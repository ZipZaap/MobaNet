---
# =====================================================
# DIRECTORIES
# =====================================================
DATASET_DIR: 
  default: D:/DataStorage/SemSeg/ver1
  type: str
  help: Path to the dataset folder.

RESULTS_DIR: 
  default: saved
  type: str
  help: Path to the experiment metadata folder.

# =====================================================
# DATASET PARAMETERS
# =====================================================
SEED:
  default: 42
  type: int
  help: Random seed for the dataset split.

TRAIN_SET_COMPOSITION: 
  default: boundary
  type: str
  choices: [full, boundary]
  help: |
    Training dataset composition:
    * full:     unfiltered dataset
    * boundary: only images containing boundaries

TEST_SET_COMPOSITION:
  default: boundary
  type: str
  choices: [full, boundary]
  help: |
    Testing dataset composition:
    * full:     unfiltered dataset
    * boundary: only images containing boundaries.

TEST_SPLIT:
  default: 0.1
  type: float
  help: Fraction of the dataset to be used as a test set.

CROSS_VALIDATION:
  default: False
  type: bool
  help: Whether to perform K-fold cross-validation.

DEFAULT_FOLD:
  default: 0
  type: int
  help: Default fold to use if cross-validation is disabled.

NUM_WORKERS:
  default: 1
  type: int
  help: Number of subprocesses to use in PyTorch DataLoader. If set to 0, data loading occurs in the main process.

# =====================================================
# MODEL CONFIGURATION
# =====================================================
MODEL: 
  default: MobaNet_ED
  type: str
  choices: [MobaNet_ED, MobaNet_EDC, MobaNet_C, MobaNet_D, UNet]
  help: |
    Type of model to be trained:
      * MobaNet_ED:  Multi-output UNet with trainable Encoder & Decoder.
      * MobaNet_EDC: Multi-output UNet with trainable Encoder, Decoder & Classification head.
      * MobaNet_C:   Multi-output UNet with trainable Classification head
      * MobaNet_D:   Multi-output UNet with trainable Decoder.
      * UNet:        Standard U-Net architecture.

CHECKPOINT:
  default: saved/exp_0/MobaNet_ED-model.pth
  type: str
  help: |
    Path to the checkpoint file containing pretrained weights.
      * When running `training`:  training will start from this checkpoint, unless `CHECKPOINT` is set to `null`.
      * When running `inference`: predictions will be made using this checkpoint; Has to be set to a valid path.

INPUT_SIZE:
  default: 512
  type: int
  help: Size of the input image.

INPUT_CHANNELS: 
  default: 1
  choices: [1, 3]
  type: int
  help: Number of channels in the input image; 1 for grayscale, 3 for RGB.

UNET_DEPTH:
  default: 5
  type: int
  help: Number of horizontal layers (or distinct resolution levels) in a UNet including the bottleneck.

CONV_DEPTH:
  default: 16
  type: int
  help: Feature depth of the Conv2D blocks in the 1st layer; this number is doubled with each downsampling stage.

BATCH_SIZE:
  default: 16
  type: int
  help: Batch size for training, inference and sdm calculation.

# --- Segmentation Branch --- #
SEG_CLASSES:
  default: 2
  type: int
  help: Number of classes in the segmentation task (incl. background); 2 is a minimum.

SEG_DROPOUT:
  default: 0.1
  type: float
  help: Dropout value for the decoder/encoder in segmentation branch.

# --- Classification Branch --- #
CLS_CLASSES:
  default: 3
  type: int
  help: Number of classes in the classification task (incl. boundary class); 3 is a minimum.

CLS_DROPOUT:
  default: 0.3
  type: float
  help: Dropout value for the classification head.

CLS_THRESHOLD:
  default: 0.7
  type: float
  help: Classification threshold; only probabilities above this yield a positive classification.

# =====================================================
# OPTIMIZER SETTINGS
# =====================================================
INIT_LR:
  default: 0.00001
  type: float
  help: Initial learning rate at the beginning of the warmup.

BASE_LR:
  default: 0.0001
  type: float
  help: Base learning rate reached after the warmup.

L2_DECAY:
  default: 0.00001
  type: float
  help: L2 regularization decay.

WARMUP_EPOCHS:
  default: 10
  type: int
  help: Number of warmup epochs to reach BASE_LR. If set to 0, no warmup is performed.

TRAIN_EPOCHS:
  default: 200
  type: int
  help: Number of training epochs, excluding the warmup.

# =====================================================
# SIGNED DISTANCE MAP (SDM) PARAMETERS
# =====================================================
SDM_KERNEL_SIZE: 
  default: 7
  type: int
  help: Kernel size for SDM estimation.

SDM_DISTANCE:
  default: chebyshev
  type: str
  choices: [manhattan, chebyshev, euclidean]
  help: Type of distance used for the SDM.

SDM_NORMALIZATION:
  default: minmax
  type: str
  choices: ['minmax', 'static_max', 'dynamic_max']
  help: |
    SDM normalisation mode. Available normalization options:
      * minmax:      by both max and min distance values of each individual SDM.
      * dynamic_max: by the max distance value of each individual SDM.
      * static_max:  by the global max distance value (depends on `SDM_DISTANCE`).

SDM_SMOOTHING:
  default: False
  type: bool
  help: |
    Whether the multi-class SDM is calculated in a smooth or discrete manner:
      * If `True`:  derived from the logits smoothly using `logsumexp`.
      * If `False`: derived from the logits using `torch.min()` operation.

SDM_SMOOTHING_ALPHA:
  default: 15.0
  type: float
  help: Smoothing factor for `logsumexp`; only relevant if `SDM_SMOOTHING == True`.

# =====================================================
# LOSS CONFIGURATION
# =====================================================
LOSS:
  default: SoftDICE
  type: str
  choices: [SoftDICE, HardDICE, IoU, SegCE, wSegCE, ClsCE, MAE, cMAE, sMAE, Boundary]
  help: | 
    Loss function or a combination of loss functions (separated by `_`) used to train the model:
      * SoftDICE: Soft (probabilistic) DICE loss  
      * HardDICE: Hard (discrete) DICE loss  
      * IoU:      Intersection-over-Union loss  
      * SegCE:    Segmentation Cross-Entropy loss  
      * wSegCE:   SDM-weighted Segmentation Cross-Entropy loss  
      * ClsCE:    Classification Cross-Entropy loss  
      * MAE:      Mean Absolute Error loss  
      * cMAE:     Clamped MAE loss  
      * sMAE:     Signed MAE loss  
      * Boundary: Boundary loss  
    For detailed information on each loss function, see `model/loss.py`.

ADAPTIVE_WEIGHTS:
  default: False
  type: bool
  help: Automatically balance contributions of loss components if combined loss is used.

STATIC_WEIGHTS:
  default: null
  type: list
  help: Static weights for the loss components. Only relevant if `ADAPTIVE_WEIGHTS == False`.
        If set to `null`, each loss is weighted equally.

CLAMP_DELTA:
  default: 0.2
  type: float
  help: Clamping delta applied to the SDM kernel when calculating `cMAE` loss.

SIGMOID_STEEPNESS:
  default: 1000
  type: int
  help: |
    Determines the steepness of the Sigmoid in `DICE`, `IoU` and `sMAE` losses.
    Higher values yield a steeper curve and a closer approximation of the step function.

# =====================================================
# EVALUATION SETTINGS
# =====================================================
SAVE_MODEL:
  default: True
  type: bool
  help: Whether to save the trained model. 

EVAL_INTERVAL:
  default: 1
  type: int
  help: Interval in epochs at which to evaluate the model on the validation set.

EVAL_METRIC:
  default: DSC
  type: str
  choices: ['TTR', 'CMA', 'DSC', 'IoU', 'ASD', 'AD', 'HD95', 'D95']
  help: |
    Best epoch selection metric:
      * TTR:  True-to-test ratio. Measures classification accuracy.
      * DSC:  DICE score. Measures global overlap.
      * IoU:  Intersection over Union score. Measures global overlap, but penalizes false positives more harshly.
      * ASD:  Average Symmetric Distance. Measures the mean distance between the boundaries of ground-truth and prediction.
      * AD:   Average one-way Distance. Relaxes the impact of false-positives.
      * HD95: Hausdorff Distance 95th percentile. Measures the worst-case boundary discrepancy.
      * D95:  One-way Distance 95th percentile. Relaxes the impact of false-positives.
      * CMA:  Combined Mean Accuracy. A weighted combination of the above.

CMA_COEFFICIENTS:
  default: {'DSC': 1, 'IoU': 1, 'ASD': 1, 'AD': 1, 'HD95': 0, 'D95': 0}
  type: dict
  help: Coefficients that define the contribution of each metric to the overall `CMA`.

DISTANCE_METRICS:
  default: True
  type: bool
  help: Whether to calculate distance metrics (ASD, AD, HD95, D95) during evaluation.

# =====================================================
# DISTRIBUTED DATA PARALLEL (DDP)
# =====================================================
GPUs:
  default: [0]
  type: list
  help: List of GPUs for distributed training; or a single GPU for non-distributed training.

MASTER_ADDR:  
  default: localhost
  type: str
  help: Master node address.

MASTER_PORT:
  default: 12355
  type: str
  help: Master node port for DDP communication.

NCCL_P2P:
  default: True
  type: bool
  help: Enable peer-to-peer communication for DDP; Disabling this might help if you encounter issues with DDP.

# =====================================================
# LOGGER SETTINGS
# =====================================================
LOG_WANDB:
  default: True
  type: bool
  help: Enable logging to Weights and Biases.

LOG_LOCAL:
  default: True
  type: bool
  help: Enable saving logs locally.

EXP_ID:
  default: exp_0
  type: str
  help: Experiment identifier. Will be generated automatically if set to `null`.

RUN_ID:
  default: null
  type: str
  help: Run identifier. Will be generated automatically if set to `null`.