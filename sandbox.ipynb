{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef922ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Configuration file passed all validation tests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.sdf import SDF\n",
    "from utils.util import load_mask, load_png, load_sdm, logits_to_lbl\n",
    "from utils.dataset import DatasetTools\n",
    "from configs.cfgparser import Config\n",
    "\n",
    "from model.MobaNet import MobaNet\n",
    "    \n",
    "\n",
    "cfg  = Config('configs/config.yaml', inference = True, cli = False)\n",
    "cfg.RANK = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f6853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DatasetTools.predict_dataloader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef13cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    ids = batch['id']\n",
    "    input = batch['image']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec72c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3e2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PREP] Generating class labels: 100%|██████████| 4933/4933 [00:06<00:00, 732.99it/s]\n"
     ]
    }
   ],
   "source": [
    "imIDs = [id.stem for id in cfg.MSK_DIR.glob('*.png')]\n",
    "\n",
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "\n",
    "for id in tqdm(imIDs, desc=\"[PREP] Generating class labels\"):\n",
    "    maskpath = cfg.MSK_DIR / f\"{id}.png\"\n",
    "    mask = load_mask(maskpath)\n",
    "\n",
    "    counts = np.bincount(mask.ravel(), minlength=cfg.SEG_CLASSES)\n",
    "    max_label = int(counts.argmax())\n",
    "    max_fraction = counts[max_label] / mask.size\n",
    "\n",
    "    if max_fraction >= 0.99:\n",
    "        lbl = max_label\n",
    "    else:\n",
    "        lbl = cfg.SEG_CLASSES\n",
    "\n",
    "    label_to_id.setdefault(str(lbl), []).append(id)\n",
    "    id_to_label[id] = lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ce9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(label_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e089783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK STATES\n",
    "# 1. UNet input:         1hot encoded; (B, C, H, W)\n",
    "# 2. SDM generation:     1hot encoded; (B, C, H, W)\n",
    "# 3. Losses DICE-based:  1hot encoded; (B, C, H, W)\n",
    "# 4. Loss sdm-based:     1hot encoded; (B, C, H, W)\n",
    "# 5. Losses CE-based:    Argmax();     (B, H, W)\n",
    "# 6. Metrics:            1hot encoded; (B, C, H, W)\n",
    "\n",
    "# SEG LOGIT STATES\n",
    "# 1. Losses DICE-based:  Probabilistic [0; 1];     (B, C, H, W)\n",
    "# 2. Loss sdm-based:     Tanh() sdm-union [-1; 1]; (B, 1, H, W)\n",
    "# 3. Losses CE-based:    Direct logits;            (B, C, H, W)\n",
    "# 4. Metrics:            1hot encoded;             (B, C, H, W)\n",
    "# 5. Visualization:      Argmax()                  (B, 1, H, W)\n",
    "\n",
    "# CLS LOGIT STATES\n",
    "# 1. Loss CE:            Direct logits; (B, C)\n",
    "# 2. Metrics:            Argmax()       (B, ) \n",
    "# 3. UNet forward():     Argmax()       (B, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 checkpoint: Path):\n",
    "\n",
    "        self.device = device\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        weights = torch.load(self.checkpoint, \n",
    "                             map_location=self.device,\n",
    "                             mmap=True,\n",
    "                             weights_only=True)['weights']\n",
    "        self.model = MobaNet(cfg)\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, input: str | Path | np.ndarray | torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts the output for the given input image.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "            input : (str | Path | np.ndarray | torch.Tensor)\n",
    "                Input image path, numpy array, or tensor.\n",
    "                - np.ndarray: should be of shape (H, W) or (H, W, C) or (B, H, W, C).\n",
    "                - torch.Tensor: should be of shape (H, W) or (H, W, C) or (B, H, W, C).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            output : torch.Tensor (B, C, H, W)\n",
    "                The model's output logits tensor.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "            ValueError\n",
    "                If the input array or tensor has an unsupported shape.\n",
    "\n",
    "            TypeError\n",
    "                If the input type is unsupported.\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(input, (str, Path)):\n",
    "            # Load image from file; (H, W, C)\n",
    "            input = load_png(input)\n",
    "\n",
    "            # add batch dimension; (H, W, C) → (1, H, W, C)\n",
    "            input = input[None, ...] \n",
    "            \n",
    "            # make pytorch compatible; (1, H, W, C) → (1, C, H, W)\n",
    "            tensor = torch.from_numpy(input).permute(0, 3, 1, 2).float() \n",
    "\n",
    "        elif isinstance(input, (np.ndarray, torch.Tensor)):\n",
    "            # Expand dimensions batch and channel dims (if necessary) \n",
    "            if input.ndim == 2:\n",
    "                input = input[None, None, ...]  # (H, W) → (1, 1, H, W)\n",
    "            elif input.ndim == 3:\n",
    "                input = input[None, ...]  # (H, W, C) → (1, H, W, C)\n",
    "            elif input.ndim > 4:\n",
    "                raise ValueError(f\"Unsupported input shape: {input.shape}. Expected an 2D, 3D or 4D (batched) input.\")\n",
    "                      \n",
    "            # Convert input to tensor if it's a numpy array and permute dimensions for PyTorch\n",
    "            tensor = torch.from_numpy(input).float() if isinstance(input, np.ndarray) else input\n",
    "            tensor = tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported input type: {type(input)}. Expected str, Path, np.ndarray, or torch.Tensor.\")\n",
    "        \n",
    "        tensor = tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.model(tensor)\n",
    "        \n",
    "    def save_predictions(self,\n",
    "                         output: torch.Tensor | np.ndarray,\n",
    "                         image: torch.Tensor | np.ndarray,\n",
    "                         imID: str,\n",
    "                         overlay: bool = True):\n",
    "        \n",
    "        \n",
    "        \n",
    "    # def save_predictions(mask: torch.Tensor, image: torch.Tensor, overlay = True) -> None:\n",
    "    #     # Save mask as 8-bit image (0, 255)\n",
    "    #     mask_8bit = (mask * 255).astype(np.uint8)\n",
    "    #     cv2.imwrite(\"mask_8bit.png\", mask_8bit)\n",
    "\n",
    "    #     if  overlay:\n",
    "    #         # Apply colormap to the mask\n",
    "    #         mask_color = cv2.applyColorMap(mask_8bit, cv2.COLORMAP_JET)\n",
    "    #         image_rgb = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    #         overlay = cv2.addWeighted(image_rgb, 0.7, mask_color, 0.3, 0)\n",
    "    #         cv2.imwrite(\"overlay.png\", overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(logits: dict[str, torch.Tensor], \n",
    "                     image: torch.Tensor, \n",
    "                     imID: str,\n",
    "                     output_dir: Path, \n",
    "                     overlay: bool = True) -> None:\n",
    "\n",
    "    pd_mask = logits['seg']\n",
    "    pd_mask = F.softmax(pd_mask, dim=1).argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "    # Save mask as 8-bit image (0, 255)\n",
    "    mask_8bit = (pd_mask * 255).astype(np.uint8)\n",
    "    mask_path = output_dir / f\"{imID}.png\"\n",
    "    cv2.imwrite(str(mask_path), mask_8bit)\n",
    "\n",
    "    if overlay:\n",
    "        # Apply colormap to the mask\n",
    "        mask_color = cv2.applyColorMap(mask_8bit, cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "        image_rgb = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        overlay = cv2.addWeighted(image_rgb, 0.7, mask_color, 0.3, 0)\n",
    "        overlay_path = output_dir / f\"{imID}_overlay.png\"\n",
    "        cv2.imwrite(str(overlay_path), overlay)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538b3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = cfg.IMG_DIR / 'ESP_018244_2655_RED-3456_0.png'\n",
    "maskpath = cfg.MSK_DIR / 'ESP_018244_2655_RED-3456_0.png'\n",
    "mask = load_mask(maskpath, C = 2)\n",
    "\n",
    "counts = np.bincount(mask.ravel(), minlength=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66846178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([262144, 262144], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb12d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impath = cfg.IMG_DIR / 'ESP_018244_2655_RED-3456_0.png'\n",
    "# img = load_png(impath)\n",
    "# # img = img[None, ...]  # add batch dimension\n",
    "# # tensor = torch.from_numpy(img).permute(0, 3, 1, 2).float() # (B, H, W, C) -> (B, C, H, W)\n",
    "# tensor = torch.from_numpy(img).permute(2, 0, 1).float()  # (H, W, C) -> (C, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a6424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-processing for a clean line: After prediction, compute the boundary as the set of pixels where a 4- or 8-connected \n",
    "# neighborhood changes class, or subtract eroded from dilated masks to get a 1-pixel ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaed61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def save_predictions(mask: torch.Tensor, image: torch.Tensor, overlay = True) -> None:\n",
    "#     # Save mask as 8-bit image (0, 255)\n",
    "#     mask_8bit = (mask * 255).astype(np.uint8)\n",
    "#     cv2.imwrite(\"mask_8bit.png\", mask_8bit)\n",
    "\n",
    "#     if  overlay:\n",
    "#         # Apply colormap to the mask\n",
    "#         mask_color = cv2.applyColorMap(mask_8bit, cv2.COLORMAP_JET)\n",
    "#         image_rgb = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "#         overlay = cv2.addWeighted(image_rgb, 0.7, mask_color, 0.3, 0)\n",
    "#         cv2.imwrite(\"overlay.png\", overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990efd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Level Description of prediction stragtegy:\n",
    "# 1. INPUT: \n",
    "#    - A direct path to an image file\n",
    "#    - A directory containing multiple image files\n",
    "#    - An image as a numpy array or tensor\n",
    "#\n",
    "# def predict(input: str | np.ndarray | torch.Tensor, cfg: Config) -> None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
